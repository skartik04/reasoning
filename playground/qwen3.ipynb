{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "94de3573",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from datasets import load_dataset\n",
    "import random\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aedbef48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95bf02208a2b47e4914f8f35808dde76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30f39b2dd87d4f8a857c4b74440d83e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d63a15830d64f5fb5c39bdc260a20a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8e9588b017f4669ae3c26f2a589218f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a464c6e636434c0e9d229f79a3fb272c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/726 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6291306fa0146cd91ea2bfda6c1e245",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe86f3f6c8424b6696da7553a1a63c48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae4e8cbacb0a4235a6ec97029aea5826",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00003.safetensors:   0%|          | 0.00/3.99G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa9bea9cb84e42acb42a6315602d1d1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00003.safetensors:   0%|          | 0.00/99.6M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc38892173d2470bad6c97be1aa510c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00003.safetensors:   0%|          | 0.00/3.96G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66ec9e8c142b47a3a94e38a53f69f839",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4795ccd281348c9a75ca3241612eaee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer loaded using cache at: /mnt/SSD4/kartik/hf_cache\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Set custom cache directory\n",
    "cache_dir = \"/mnt/SSD4/kartik/hf_cache\"\n",
    "\n",
    "# Load tokenizer from custom cache directory\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"Qwen/Qwen3-4B\",\n",
    "    cache_dir=cache_dir\n",
    ")\n",
    "\n",
    "# Load model from custom cache directory\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"Qwen/Qwen3-4B\",\n",
    "    cache_dir=cache_dir,\n",
    "    torch_dtype=torch.float16,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "print(f\"Model and tokenizer loaded using cache at: {cache_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "64052efa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4c989d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "QWEN_CHAT_TEMPLATE = \"<|im_start|>user\\n{prompt}\\n<|im_end|>\\n<|im_start|>assistant\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5a71e9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "QWEN_CHAT_TEMPLATE_NO = \"<|im_start|>user\\n{prompt}\\n<|im_end|>\\n<|im_start|>assistant\\n<think>\\n\\n</think>\\n\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f81901b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_prompt(prompt):\n",
    "    return QWEN_CHAT_TEMPLATE.format(prompt=prompt)\n",
    "\n",
    "def tokenize_input(text):\n",
    "    \"\"\"Tokenize the input text and return input_ids and attention_mask.\"\"\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    return inputs\n",
    "\n",
    "def generate_response(formatted_prompt, max_new_tokens=1000):\n",
    "    inputs = tokenize_input(formatted_prompt).to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    response = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5ccf12c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_final_answer(text):\n",
    "    # Try strict match first: 'Answer: <number>'\n",
    "    match = re.search(r'Answer:\\s*\\$?(\\d+(?:\\.\\d+)?)', text, re.IGNORECASE)\n",
    "    if match:\n",
    "        return int(float(match.group(1)))\n",
    "\n",
    "    # Try to extract from LaTeX-style boxed answer\n",
    "    match = re.search(r'\\\\boxed{?\\$?(\\d+(?:\\.\\d+)?)', text)\n",
    "    if match:\n",
    "        return int(float(match.group(1)))\n",
    "\n",
    "    # As fallback, look for last number in text\n",
    "    all_nums = re.findall(r'\\d+(?:\\.\\d+)?', text)\n",
    "    if all_nums:\n",
    "        return int(float(all_nums[-1]))\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e9dad289",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"gsm8k\", \"main\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "003488b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 30\n",
    "random.seed(0)\n",
    "indices = random.sample(range(len(dataset['train'])), n)\n",
    "qa_pairs = dataset['train'][indices]\n",
    "questions = qa_pairs['question']\n",
    "answers = qa_pairs['answer']\n",
    "\n",
    "import re\n",
    "final_answers = []\n",
    "for answer in answers:\n",
    "    numbers = re.findall(r'\\d+', answer)\n",
    "    final_answers.append(int(numbers[-1]) if numbers else '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1f6057bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction = (\n",
    "    \"Give ONLY the final numeric answer. Do NOT explain or show your work. \"\n",
    "    \"Write nothing except the answer.\\n\\n\"\n",
    "    \"Answer:\"\n",
    ")\n",
    "\n",
    "def instructions(thinking=True):\n",
    "    nothink_token =  \"/nothink\"\n",
    "    if thinking:\n",
    "        return instruction\n",
    "    else:\n",
    "        return instruction + nothink_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "189faa71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:14<00:00, 14.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'The state of Virginia had 3.79 inches of rain in March, 4.5 inches of rain in April, 3.95 inches of rain in May, 3.09 inches of rain in June and 4.67 inches in July.  What is the average rainfall amount, in inches, in Virginia?', 'response_thinking': \"user\\nThe state of Virginia had 3.79 inches of rain in March, 4.5 inches of rain in April, 3.95 inches of rain in May, 3.09 inches of rain in June and 4.67 inches in July.  What is the average rainfall amount, in inches, in Virginia?Give ONLY the final numeric answer. Do NOT explain or show your work. Write nothing except the answer.\\n\\nAnswer:\\n\\nassistant\\n<think>\\nOkay, let me try to figure out the average rainfall for Virginia. So, they gave me the rainfall amounts for each month from March to July. Let me list them out again to make sure I have them all:\\n\\nMarch: 3.79 inches  \\nApril: 4.5 inches  \\nMay: 3.95 inches  \\nJune: 3.09 inches  \\nJuly: 4.67 inches  \\n\\nFirst, I need to find the total rainfall for these five months and then divide by 5 to get the average. Let me add them up step by step. \\n\\nStarting with March and April: 3.79 + 4.5. Hmm, 3.79 plus 4.5. Let me convert 4.5 to 4.50 to make it easier. So 3.79 + 4.50 = 8.29 inches. \\n\\nNext, add May's rainfall: 8.29 + 3.95. Let me do that. 8.29 + 3.95. Adding the decimals first: 0.29 + 0.95 = 1.24. Then 8 + 3 = 11. So total is 11 + 1.24 = 12.24 inches. \\n\\nNow add June's 3.09 inches: 12.24 + 3.09. Let's see, 12.24 + 3.09. 12 + 3 = 15, 0.24 + 0.09 = 0.33. So total is 15.33 inches. \\n\\nThen add July's 4.67 inches: 15.33 + 4.67. Wait, that seems like a nice round number. 15.33 + 4.67 = 20 inches exactly. \\n\\nSo the total rainfall over the five months is 20 inches. To find the average, divide by 5: 20 / 5 = 4 inches. \\n\\nWait, that seems too round. Let me check my addition again to make sure I didn't make a mistake. \\n\\nMarch: 3.79  \\nApril: 4.50  \\nMay: \", 'response_nothinking': 'user\\nThe state of Virginia had 3.79 inches of rain in March, 4.5 inches of rain in April, 3.95 inches of rain in May, 3.09 inches of rain in June and 4.67 inches in July.  What is the average rainfall amount, in inches, in Virginia?Give ONLY the final numeric answer. Do NOT explain or show your work. Write nothing except the answer.\\n\\nAnswer:/nothink\\n\\nassistant\\n<think>\\n\\n</think>\\n\\n5.18', 'predicted_thinking': 4, 'predicted_nothinking': 5, 'answer': 4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "max_new_tokens = 500\n",
    "for i, question in enumerate(tqdm(questions[:1])):\n",
    "    result = {\n",
    "        \"question\": question,\n",
    "    }\n",
    "\n",
    "    # Thinking mode\n",
    "    prompt_think = question + instructions(thinking=True)\n",
    "    formatted_think = format_prompt(prompt_think)\n",
    "    response_think = generate_response(formatted_think, max_new_tokens=max_new_tokens)\n",
    "    result[\"response_thinking\"] = response_think\n",
    "    \n",
    "    # Nothinking mode\n",
    "    prompt_nothink = question + instructions(thinking=False)\n",
    "    formatted_nothink = format_prompt(prompt_nothink)\n",
    "    response_nothink = generate_response(formatted_nothink, max_new_tokens=max_new_tokens)\n",
    "    result[\"response_nothinking\"] = response_nothink\n",
    "\n",
    "    result[\"predicted_thinking\"] = extract_final_answer(response_think)\n",
    "    result[\"predicted_nothinking\"] = extract_final_answer(response_nothink)\n",
    "    result['answer'] = final_answers[i]\n",
    "\n",
    "    results.append(result)\n",
    "\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "727326f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = json.load(open(\"qwen3_concise.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1100cba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thinking: 19/20, Nothinking: 14/20\n",
      "Thinking accuracy: 0.95\n",
      "Nothinking accuracy: 0.7\n"
     ]
    }
   ],
   "source": [
    "think = 0\n",
    "nothink = 0\n",
    "for i in range(len(results)):\n",
    "    if results[i]['predicted_thinking'] == results[i]['answer']:\n",
    "        think += 1\n",
    "    if results[i]['predicted_nothinking'] == results[i]['answer']:\n",
    "        nothink += 1\n",
    "\n",
    "print(f\"Thinking: {think}/{len(results)}, Nothinking: {nothink}/{len(results)}\")\n",
    "print(f\"Thinking accuracy: {think / len(results)}\")\n",
    "print(f\"Nothinking accuracy: {nothink / len(results)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "358e446e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca6808d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct: 14/20\n",
      "Accuracy: 0.7\n"
     ]
    }
   ],
   "source": [
    "force_no = json.load(open(\"qwen3_concise_no.json\"))\n",
    "correct = 0\n",
    "for i in range(len(force_no)):\n",
    "    if force_no[i]['predicted_nothinking'] == force_no[i]['answer']:\n",
    "        correct += 1\n",
    "print(f\"Correct: {correct}/{len(force_no)}\")\n",
    "print(f\"Accuracy: {correct / len(force_no)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2986369c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Think: 16/ 20, Nothink: 11/ 20\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "forced = json.load(open('qwen3_forced_answer.json'))[:20]\n",
    "think = 0\n",
    "nothink = 0\n",
    "\n",
    "def is_correct(pred, target):\n",
    "    if pred is None:\n",
    "        return False\n",
    "    try:\n",
    "        pred_val = float(pred)\n",
    "        return pred_val == int(target)\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "for i in range(len(forced)):\n",
    "    if is_correct(forced[i]['predicted_thinking'], forced[i]['answer']):\n",
    "        think += 1\n",
    "    if is_correct(forced[i]['predicted_nothinking'], forced[i]['answer']):\n",
    "        nothink += 1\n",
    "\n",
    "# print(f\"Think: {think}, Nothink: {nothink}\")\n",
    "tot = len(forced)\n",
    "print(f\"Think: {think}/ {tot}, Nothink: {nothink}/ {tot}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44addb18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uv-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
