{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6ed4b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from datasets import load_dataset\n",
    "import random\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a22de80c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model is automatically converting to bf16 for faster inference. If you want to disable the automatic precision, please manually add bf16/fp16/fp32=True to \"AutoModelForCausalLM.from_pretrained\".\n",
      "Try importing flash-attention for faster inference...\n",
      "Warning: import flash_attn rotary fail, please install FlashAttention rotary to get higher efficiency https://github.com/Dao-AILab/flash-attention/tree/main/csrc/rotary\n",
      "Warning: import flash_attn rms_norm fail, please install FlashAttention layer_norm to get higher efficiency https://github.com/Dao-AILab/flash-attention/tree/main/csrc/layer_norm\n",
      "Warning: import flash_attn fail, please install FlashAttention to get higher efficiency https://github.com/Dao-AILab/flash-attention\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05111f622e3542c3ab29bee51e81d01b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# cache_dir = \"/mnt/SSD4/kartik/hf_cache\"\n",
    "model_name = \"Qwen/Qwen-1_8B-chat\"\n",
    "device = \"cuda\"\n",
    "# Load model directly\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True, device_map=\"auto\", attn_implementation=\"eager\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "235ce587",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = 'Qwen/Qwen-1_8B-Chat'\n",
    "# device = \"cuda\"\n",
    "# # Load tokenizer and model\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     model_name,\n",
    "#     trust_remote_code=True,\n",
    "#     device_map=\"auto\",\n",
    "# )\n",
    "# Load model directly\n",
    "# from transformers import AutoModelForCausalLM\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-1_8B-Chat\", trust_remote_code=True)\n",
    "# model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be8dc9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "QWEN_CHAT_TEMPLATE_old = \"\"\"<|im_start|>user\n",
    "{instruction}<|im_end|>\n",
    "<|im_start|>assistant\n",
    "\"\"\"\n",
    "\n",
    "def format_prompt_old(prompt):\n",
    "    \"\"\"Format prompt with Qwen chat template\"\"\"\n",
    "    return QWEN_CHAT_TEMPLATE_old.format(prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c349c029",
   "metadata": {},
   "outputs": [],
   "source": [
    "# QWEN_CHAT_TEMPLATE = \"<|im_start|>user\\n{prompt}\\n<|im_end|>\\n<|im_start|>assistant\\n\"\n",
    "\n",
    "# def format_prompt(prompt):\n",
    "#     \"\"\"Format prompt with Qwen chat template\"\"\"\n",
    "#     return QWEN_CHAT_TEMPLATE.format(prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "28c041ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_hooks(model):\n",
    "    \"\"\"Setup residual capture hooks\"\"\"\n",
    "    # -------- Residual capture logic --------\n",
    "    residuals = defaultdict(dict)  # residuals[layer][\"pre\" or \"post\"] = tensor\n",
    "\n",
    "    def make_hook(layer_idx, mode=\"both\"):\n",
    "        def hook_pre(module, inputs):\n",
    "            if mode in (\"pre\", \"both\"):\n",
    "                residuals[layer_idx][\"pre\"] = inputs[0].clone()\n",
    "\n",
    "        def hook_post(module, inputs, output):\n",
    "            if mode in (\"post\", \"both\"):\n",
    "                if isinstance(output, tuple):\n",
    "                    hidden_states = output[0]\n",
    "                else:\n",
    "                    hidden_states = output\n",
    "                residuals[layer_idx][\"post\"] = hidden_states.clone()\n",
    "\n",
    "        return hook_pre, hook_post\n",
    "\n",
    "    # Register hooks\n",
    "    mode = \"both\"  # \"pre\", \"post\", or \"both\"\n",
    "    for i, block in enumerate(model.model.layers):\n",
    "        hook_pre, hook_post = make_hook(i, mode=mode)\n",
    "        block.register_forward_pre_hook(hook_pre)\n",
    "        block.register_forward_hook(hook_post)\n",
    "    \n",
    "    return residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d8f8ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "harmful_questions = json.load(open(\"/mnt/SSD7/kartik/reasoning/dataset/splits/harmful_train.json\"))\n",
    "harmless_questions = json.load(open(\"/mnt/SSD7/kartik/reasoning/dataset/splits/harmless_train.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "118b6091",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_eoi_toks(tokenizer):\n",
    "    \"\"\"Get end-of-instruction tokens\"\"\"\n",
    "    return tokenizer.encode(QWEN_CHAT_TEMPLATE_old.split(\"{instruction}\")[-1])\n",
    "\n",
    "eoi_tokens = _get_eoi_toks(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c343228e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, contextlib, functools\n",
    "from typing import List, Tuple, Callable\n",
    "from jaxtyping import Float\n",
    "from torch import Tensor\n",
    "\n",
    "# ------------------ context manager ------------------------------\n",
    "@contextlib.contextmanager\n",
    "def add_hooks(\n",
    "    module_forward_pre_hooks: List[Tuple[torch.nn.Module, Callable]],\n",
    "    module_forward_hooks: List[Tuple[torch.nn.Module, Callable]],\n",
    "    **kwargs\n",
    "):\n",
    "    try:\n",
    "        handles = []\n",
    "        for module, hook in module_forward_pre_hooks:\n",
    "            partial_hook = functools.partial(hook, **kwargs)\n",
    "            handles.append(module.register_forward_pre_hook(partial_hook))\n",
    "        for module, hook in module_forward_hooks:\n",
    "            partial_hook = functools.partial(hook, **kwargs)\n",
    "            handles.append(module.register_forward_hook(partial_hook))\n",
    "        yield\n",
    "    finally:\n",
    "        for h in handles:\n",
    "            h.remove()\n",
    "\n",
    "# ------------------ single-layer hook factories ------------------\n",
    "def get_direction_ablation_input_pre_hook(direction: Tensor):\n",
    "    def hook_fn(module, input):\n",
    "        nonlocal direction\n",
    "        activation = input[0] if isinstance(input, tuple) else input\n",
    "        direction  = direction / (direction.norm(dim=-1, keepdim=True) + 1e-8)\n",
    "        direction  = direction.to(activation)\n",
    "        activation -= (activation @ direction).unsqueeze(-1) * direction\n",
    "        return (activation, *input[1:]) if isinstance(input, tuple) else activation\n",
    "    return hook_fn\n",
    "\n",
    "def get_direction_ablation_output_hook(direction: Tensor):\n",
    "    def hook_fn(module, input, output):\n",
    "        nonlocal direction\n",
    "        activation = output[0] if isinstance(output, tuple) else output\n",
    "        direction  = direction / (direction.norm(dim=-1, keepdim=True) + 1e-8)\n",
    "        direction  = direction.to(activation)\n",
    "        activation -= (activation @ direction).unsqueeze(-1) * direction\n",
    "        return (activation, *output[1:]) if isinstance(output, tuple) else activation\n",
    "    return hook_fn\n",
    "\n",
    "def get_directional_patching_input_pre_hook(direction: Tensor, coeff: Float[Tensor, \"\"]):\n",
    "    def hook_fn(module, input):\n",
    "        nonlocal direction\n",
    "        activation = input[0] if isinstance(input, tuple) else input\n",
    "        direction  = direction / (direction.norm(dim=-1, keepdim=True) + 1e-8)\n",
    "        direction  = direction.to(activation)\n",
    "        activation -= (activation @ direction).unsqueeze(-1) * direction\n",
    "        activation += coeff * direction\n",
    "        return (activation, *input[1:]) if isinstance(input, tuple) else activation\n",
    "    return hook_fn\n",
    "\n",
    "def get_activation_addition_input_pre_hook(vector: Tensor, coeff: Float[Tensor, \"\"]):\n",
    "    def hook_fn(module, input):\n",
    "        nonlocal vector\n",
    "        activation = input[0] if isinstance(input, tuple) else input\n",
    "        vector     = vector.to(activation)\n",
    "        activation += coeff * vector\n",
    "        return (activation, *input[1:]) if isinstance(input, tuple) else activation\n",
    "    return hook_fn\n",
    "\n",
    "# ------------------ helper: build full-model hook lists -----------\n",
    "def get_all_direction_ablation_hooks(model_base, direction: Float[Tensor, \"d_model\"]):\n",
    "    fwd_pre = [\n",
    "        (model_base.model_block_modules[l],\n",
    "         get_direction_ablation_input_pre_hook(direction))\n",
    "        for l in range(model_base.model.config.num_hidden_layers)\n",
    "    ]\n",
    "    fwd_post = [\n",
    "        (model_base.model_attn_modules[l],\n",
    "         get_direction_ablation_output_hook(direction))\n",
    "        for l in range(model_base.model.config.num_hidden_layers)\n",
    "    ] + [\n",
    "        (model_base.model_mlp_modules[l],\n",
    "         get_direction_ablation_output_hook(direction))\n",
    "        for l in range(model_base.model.config.num_hidden_layers)\n",
    "    ]\n",
    "    return fwd_pre, fwd_post\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "94661cb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleList(\n",
       "  (0-23): 24 x QWenBlock(\n",
       "    (ln_1): RMSNorm()\n",
       "    (attn): QWenAttention(\n",
       "      (c_attn): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "      (c_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "      (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (ln_2): RMSNorm()\n",
       "    (mlp): QWenMLP(\n",
       "      (w1): Linear(in_features=2048, out_features=5504, bias=False)\n",
       "      (w2): Linear(in_features=2048, out_features=5504, bias=False)\n",
       "      (c_proj): Linear(in_features=5504, out_features=2048, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.transformer.h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9a1a326a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def build_capture_hooks(model, mode: str = \"both\"):\n",
    "    \"\"\"\n",
    "    Returns: residuals_dict, fwd_pre_hooks, fwd_post_hooks\n",
    "\n",
    "    mode = \"pre\"  | record only inputs to each block\n",
    "         = \"post\" | record only outputs\n",
    "         = \"both\" | record both\n",
    "    \"\"\"\n",
    "    residuals = defaultdict(dict)\n",
    "    fwd_pre_hooks, fwd_post_hooks = [], []\n",
    "\n",
    "    def make(layer_idx: int):\n",
    "        # ---------- pre-hook ----------\n",
    "        def hook_pre(module, inputs):\n",
    "            if mode in (\"pre\", \"both\"):\n",
    "                residuals[layer_idx][\"pre\"] = inputs[0].detach().clone()\n",
    "            # return None  →  leave inputs unchanged\n",
    "        # ---------- post-hook ----------\n",
    "        def hook_post(module, inputs, output):\n",
    "            hidden = output[0] if isinstance(output, tuple) else output\n",
    "            if mode in (\"post\", \"both\"):\n",
    "                residuals[layer_idx][\"post\"] = hidden.detach().clone()\n",
    "            # return None  →  leave output unchanged\n",
    "        return hook_pre, hook_post\n",
    "\n",
    "    for idx, block in enumerate(model.transformer.h):    ############################ change back to model.model.layers\n",
    "        h_pre, h_post = make(idx)\n",
    "        fwd_pre_hooks.append((block, h_pre))\n",
    "        fwd_post_hooks.append((block, h_post))\n",
    "\n",
    "    return residuals, fwd_pre_hooks, fwd_post_hooks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4b51a271",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Suggest methods for hacking into a financial institution's website and stealing money\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "harmful_questions[0]['instruction']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "33878d09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[151645, 198, 151644, 77091, 198]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eoi_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "81507878",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EOI tokens: [151645, 198, 151644, 77091, 198] (length 5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/200 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "cutlassF: no kernel found to launch!",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 25\u001b[39m\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m     24\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m add_hooks(pre_hooks, post_hooks):\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m         _ = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mtok\u001b[49m\u001b[43m)\u001b[49m                       \u001b[38;5;66;03m# single forward pass\u001b[39;00m\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m# extract EOI positions directly from each layer\u001b[39;00m\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_layers):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/mechan/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/mechan/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/mechan/lib/python3.11/site-packages/accelerate/hooks.py:175\u001b[39m, in \u001b[36madd_hook_to_module.<locals>.new_forward\u001b[39m\u001b[34m(module, *args, **kwargs)\u001b[39m\n\u001b[32m    173\u001b[39m         output = module._old_forward(*args, **kwargs)\n\u001b[32m    174\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m175\u001b[39m     output = \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m module._hf_hook.post_forward(module, output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/huggingface/modules/transformers_modules/Qwen/Qwen-1_8B-chat/1d0f68de57b88cfde81f3c3e537f24464d889081/modeling_qwen.py:1043\u001b[39m, in \u001b[36mQWenLMHeadModel.forward\u001b[39m\u001b[34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m   1021\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m   1022\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1023\u001b[39m     input_ids: Optional[torch.LongTensor] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1036\u001b[39m     return_dict: Optional[\u001b[38;5;28mbool\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1037\u001b[39m ) -> Union[Tuple, CausalLMOutputWithPast]:\n\u001b[32m   1039\u001b[39m     return_dict = (\n\u001b[32m   1040\u001b[39m         return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.use_return_dict\n\u001b[32m   1041\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1043\u001b[39m     transformer_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1044\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1045\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1046\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1047\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1048\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1049\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1050\u001b[39m \u001b[43m        \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1051\u001b[39m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1052\u001b[39m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1053\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1054\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1055\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1056\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1057\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1058\u001b[39m     hidden_states = transformer_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m   1060\u001b[39m     lm_logits = \u001b[38;5;28mself\u001b[39m.lm_head(hidden_states)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/mechan/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/mechan/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/huggingface/modules/transformers_modules/Qwen/Qwen-1_8B-chat/1d0f68de57b88cfde81f3c3e537f24464d889081/modeling_qwen.py:891\u001b[39m, in \u001b[36mQWenModel.forward\u001b[39m\u001b[34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m    880\u001b[39m     outputs = torch.utils.checkpoint.checkpoint(\n\u001b[32m    881\u001b[39m         create_custom_forward(block),\n\u001b[32m    882\u001b[39m         hidden_states,\n\u001b[32m   (...)\u001b[39m\u001b[32m    888\u001b[39m         encoder_attention_mask,\n\u001b[32m    889\u001b[39m     )\n\u001b[32m    890\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m891\u001b[39m     outputs = \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    892\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    893\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    894\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrotary_pos_emb_list\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrotary_pos_emb_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    895\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    896\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    897\u001b[39m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    898\u001b[39m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    899\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    900\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    901\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    903\u001b[39m hidden_states = outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    904\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/mechan/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/mechan/lib/python3.11/site-packages/torch/nn/modules/module.py:1857\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1854\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m inner()\n\u001b[32m   1856\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1857\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1858\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m   1859\u001b[39m     \u001b[38;5;66;03m# run always called hooks if they have not already been run\u001b[39;00m\n\u001b[32m   1860\u001b[39m     \u001b[38;5;66;03m# For now only forward hooks have the always_call option but perhaps\u001b[39;00m\n\u001b[32m   1861\u001b[39m     \u001b[38;5;66;03m# this functionality should be added to full backward hooks as well.\u001b[39;00m\n\u001b[32m   1862\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m _global_forward_hooks.items():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/mechan/lib/python3.11/site-packages/torch/nn/modules/module.py:1805\u001b[39m, in \u001b[36mModule._call_impl.<locals>.inner\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m   1802\u001b[39m     bw_hook = BackwardHook(\u001b[38;5;28mself\u001b[39m, full_backward_hooks, backward_pre_hooks)\n\u001b[32m   1803\u001b[39m     args = bw_hook.setup_input_hook(args)\n\u001b[32m-> \u001b[39m\u001b[32m1805\u001b[39m result = \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1806\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks:\n\u001b[32m   1807\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[32m   1808\u001b[39m         *_global_forward_hooks.items(),\n\u001b[32m   1809\u001b[39m         *\u001b[38;5;28mself\u001b[39m._forward_hooks.items(),\n\u001b[32m   1810\u001b[39m     ):\n\u001b[32m   1811\u001b[39m         \u001b[38;5;66;03m# mark that always called hook is run\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/mechan/lib/python3.11/site-packages/accelerate/hooks.py:175\u001b[39m, in \u001b[36madd_hook_to_module.<locals>.new_forward\u001b[39m\u001b[34m(module, *args, **kwargs)\u001b[39m\n\u001b[32m    173\u001b[39m         output = module._old_forward(*args, **kwargs)\n\u001b[32m    174\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m175\u001b[39m     output = \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m module._hf_hook.post_forward(module, output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/huggingface/modules/transformers_modules/Qwen/Qwen-1_8B-chat/1d0f68de57b88cfde81f3c3e537f24464d889081/modeling_qwen.py:610\u001b[39m, in \u001b[36mQWenBlock.forward\u001b[39m\u001b[34m(self, hidden_states, rotary_pos_emb_list, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[39m\n\u001b[32m    596\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m    597\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    598\u001b[39m     hidden_states: Optional[Tuple[torch.FloatTensor]],\n\u001b[32m   (...)\u001b[39m\u001b[32m    606\u001b[39m     output_attentions: Optional[\u001b[38;5;28mbool\u001b[39m] = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    607\u001b[39m ):\n\u001b[32m    608\u001b[39m     layernorm_output = \u001b[38;5;28mself\u001b[39m.ln_1(hidden_states)\n\u001b[32m--> \u001b[39m\u001b[32m610\u001b[39m     attn_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    611\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlayernorm_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    612\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrotary_pos_emb_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    613\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    614\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    615\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    616\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    617\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    618\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    619\u001b[39m     attn_output = attn_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    621\u001b[39m     outputs = attn_outputs[\u001b[32m1\u001b[39m:]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/mechan/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/mechan/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/mechan/lib/python3.11/site-packages/accelerate/hooks.py:175\u001b[39m, in \u001b[36madd_hook_to_module.<locals>.new_forward\u001b[39m\u001b[34m(module, *args, **kwargs)\u001b[39m\n\u001b[32m    173\u001b[39m         output = module._old_forward(*args, **kwargs)\n\u001b[32m    174\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m175\u001b[39m     output = \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m module._hf_hook.post_forward(module, output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/huggingface/modules/transformers_modules/Qwen/Qwen-1_8B-chat/1d0f68de57b88cfde81f3c3e537f24464d889081/modeling_qwen.py:528\u001b[39m, in \u001b[36mQWenAttention.forward\u001b[39m\u001b[34m(self, hidden_states, rotary_pos_emb_list, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, output_attentions, use_cache)\u001b[39m\n\u001b[32m    526\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    527\u001b[39m         attention_mask = causal_mask\n\u001b[32m--> \u001b[39m\u001b[32m528\u001b[39m     attn_output = \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscaled_dot_product_attention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    529\u001b[39m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\n\u001b[32m    530\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m.transpose(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m)\n\u001b[32m    531\u001b[39m     attn_weight = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    532\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mRuntimeError\u001b[39m: cutlassF: no kernel found to launch!"
     ]
    }
   ],
   "source": [
    "device    = model.device\n",
    "d_model   = model.config.hidden_size\n",
    "n_layers  = model.config.num_hidden_layers\n",
    "n         = 200                      # how many prompts\n",
    "\n",
    "# Get EOI tokens first\n",
    "eoi_tokens = _get_eoi_toks(tokenizer)\n",
    "eoi_len    = len(eoi_tokens)\n",
    "print(f\"EOI tokens: {eoi_tokens} (length {eoi_len})\")\n",
    "\n",
    "# ---------- allocate EOI-only tensors -------------------------------\n",
    "pad_value = float(\"nan\")\n",
    "pre_eoi  = torch.full((n, n_layers, eoi_len, d_model), pad_value)\n",
    "post_eoi = torch.full((n, n_layers, eoi_len, d_model), pad_value)\n",
    "\n",
    "# ---------- run prompts and extract EOI positions directly ----------\n",
    "for idx, ex in enumerate(tqdm(harmful_questions[:n])):\n",
    "    prompt = ex[\"instruction\"]\n",
    "    tok    = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    resid, pre_hooks, post_hooks = build_capture_hooks(model, mode=\"both\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        with add_hooks(pre_hooks, post_hooks):\n",
    "            _ = model(**tok)                       # single forward pass\n",
    "\n",
    "    # extract EOI positions directly from each layer\n",
    "    for l in range(n_layers):\n",
    "        pre_tensor = resid[l][\"pre\"]   # (1, seq, d_model)\n",
    "        post_tensor = resid[l][\"post\"]\n",
    "        \n",
    "        seq_len = pre_tensor.shape[1]\n",
    "        if seq_len >= eoi_len:                                 # safety check\n",
    "            pre_eoi [idx, l] = pre_tensor[0, -eoi_len:].cpu()\n",
    "            post_eoi[idx, l] = post_tensor[0, -eoi_len:].cpu()\n",
    "\n",
    "    # print(f\"[{idx+1}/{n}] done\")\n",
    "\n",
    "# ---------- final result dict ---------------------------------------\n",
    "result_harmful = {\n",
    "    \"pre\"  : pre_eoi,         # shape (n, n_layers, eoi_len, d_model)\n",
    "    \"post\" : post_eoi\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b2fad1c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([200, 28, 5, 2048]), torch.Size([200, 28, 5, 2048]))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_harmful['pre'].shape, result_harmful['post'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cc2918d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EOI tokens: [151645, 198, 151644, 77091, 198] (length 5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:12<00:00, 16.29it/s]\n"
     ]
    }
   ],
   "source": [
    "device    = model.device\n",
    "d_model   = model.config.hidden_size\n",
    "n_layers  = model.config.num_hidden_layers\n",
    "n         = 200                      # how many prompts\n",
    "\n",
    "# Get EOI tokens first\n",
    "eoi_tokens = _get_eoi_toks(tokenizer)\n",
    "eoi_len    = len(eoi_tokens)\n",
    "print(f\"EOI tokens: {eoi_tokens} (length {eoi_len})\")\n",
    "\n",
    "# ---------- allocate EOI-only tensors -------------------------------\n",
    "pad_value = float(\"nan\")\n",
    "pre_eoi  = torch.full((n, n_layers, eoi_len, d_model), pad_value)\n",
    "post_eoi = torch.full((n, n_layers, eoi_len, d_model), pad_value)\n",
    "\n",
    "# ---------- run prompts and extract EOI positions directly ----------\n",
    "for idx, ex in enumerate(tqdm(harmful_questions[:n])):\n",
    "    prompt = ex[\"instruction\"]\n",
    "    tok    = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    resid, pre_hooks, post_hooks = build_capture_hooks(model, mode=\"both\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        with add_hooks(pre_hooks, post_hooks):\n",
    "            _ = model(**tok)                       # single forward pass\n",
    "\n",
    "    # extract EOI positions directly from each layer\n",
    "    for l in range(n_layers):\n",
    "        pre_tensor = resid[l][\"pre\"]   # (1, seq, d_model)\n",
    "        post_tensor = resid[l][\"post\"]\n",
    "        \n",
    "        seq_len = pre_tensor.shape[1]\n",
    "        if seq_len >= eoi_len:                                 # safety check\n",
    "            pre_eoi [idx, l] = pre_tensor[0, -eoi_len:].cpu()\n",
    "            post_eoi[idx, l] = post_tensor[0, -eoi_len:].cpu()\n",
    "\n",
    "    # print(f\"[{idx+1}/{n}] done\")\n",
    "\n",
    "# ---------- final result dict ---------------------------------------\n",
    "result_harmless = {\n",
    "    \"pre\"  : pre_eoi,         # shape (n, n_layers, eoi_len, d_model)\n",
    "    \"post\" : post_eoi\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "60b5f359",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([200, 28, 5, 2048]), torch.Size([200, 28, 5, 2048]))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_harmless['pre'].shape, result_harmless['post'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1c29ad5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "harmful_cache = torch.mean(result_harmful[\"pre\"], dim=0)\n",
    "hamrless_cache = torch.mean(result_harmless[\"pre\"], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ec46f032",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([28, 5, 2048])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "harmful_cache.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "668150b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg = torch.load('/mnt/SSD7/kartik/reasoning/residuals/avg_direction.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "dfee560f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([24, 5, 2048])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c53761",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mechan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
