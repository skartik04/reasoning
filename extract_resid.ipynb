{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57a08486",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from datasets import load_dataset\n",
    "import random\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6cf8f858",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The repository `Qwen/Qwen-1_8B-Chat` contains custom code which must be executed to correctly load the model. You can inspect the repository content at https://hf.co/Qwen/Qwen-1_8B-Chat.\nPlease pass the argument `trust_remote_code=True` to allow custom code to be run.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m device = \u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Load tokenizer and model\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m tokenizer = \u001b[43mAutoTokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m model = AutoModelForCausalLM.from_pretrained(\n\u001b[32m      6\u001b[39m     model_name,\n\u001b[32m      7\u001b[39m     trust_remote_code=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m      8\u001b[39m     device_map=\u001b[33m\"\u001b[39m\u001b[33mauto\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      9\u001b[39m )\n\u001b[32m     10\u001b[39m model.eval()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/mechan/lib/python3.11/site-packages/transformers/models/auto/tokenization_auto.py:985\u001b[39m, in \u001b[36mAutoTokenizer.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[39m\n\u001b[32m    977\u001b[39m has_remote_code = tokenizer_auto_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    978\u001b[39m has_local_code = \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m TOKENIZER_MAPPING \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m    979\u001b[39m     config_tokenizer_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    980\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (\n\u001b[32m   (...)\u001b[39m\u001b[32m    983\u001b[39m     )\n\u001b[32m    984\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m985\u001b[39m trust_remote_code = \u001b[43mresolve_trust_remote_code\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    986\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhas_local_code\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhas_remote_code\u001b[49m\n\u001b[32m    987\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    989\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_remote_code \u001b[38;5;129;01mand\u001b[39;00m trust_remote_code:\n\u001b[32m    990\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m use_fast \u001b[38;5;129;01mand\u001b[39;00m tokenizer_auto_map[\u001b[32m1\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/mechan/lib/python3.11/site-packages/transformers/dynamic_module_utils.py:731\u001b[39m, in \u001b[36mresolve_trust_remote_code\u001b[39m\u001b[34m(trust_remote_code, model_name, has_local_code, has_remote_code, error_message)\u001b[39m\n\u001b[32m    728\u001b[39m         _raise_timeout_error(\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    730\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_remote_code \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m has_local_code \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m trust_remote_code:\n\u001b[32m--> \u001b[39m\u001b[32m731\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    732\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00merror_message\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m You can inspect the repository content at https://hf.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    733\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mPlease pass the argument `trust_remote_code=True` to allow custom code to be run.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    734\u001b[39m     )\n\u001b[32m    736\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m trust_remote_code\n",
      "\u001b[31mValueError\u001b[39m: The repository `Qwen/Qwen-1_8B-Chat` contains custom code which must be executed to correctly load the model. You can inspect the repository content at https://hf.co/Qwen/Qwen-1_8B-Chat.\nPlease pass the argument `trust_remote_code=True` to allow custom code to be run."
     ]
    }
   ],
   "source": [
    "model_name = 'Qwen/Qwen-1_8B-Chat'\n",
    "device = \"cuda\"\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b630b7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "QWEN_CHAT_TEMPLATE_old = \"\"\"<|im_start|>user\n",
    "{instruction}<|im_end|>\n",
    "<|im_start|>assistant\n",
    "\"\"\"\n",
    "\n",
    "def format_prompt_old(prompt):\n",
    "    \"\"\"Format prompt with Qwen chat template\"\"\"\n",
    "    return QWEN_CHAT_TEMPLATE_old.format(prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82ec24b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, contextlib, functools\n",
    "from typing import List, Tuple, Callable\n",
    "from jaxtyping import Float\n",
    "from torch import Tensor\n",
    "\n",
    "# ------------------ context manager ------------------------------\n",
    "@contextlib.contextmanager\n",
    "def add_hooks(\n",
    "    module_forward_pre_hooks: List[Tuple[torch.nn.Module, Callable]],\n",
    "    module_forward_hooks: List[Tuple[torch.nn.Module, Callable]],\n",
    "    **kwargs\n",
    "):\n",
    "    try:\n",
    "        handles = []\n",
    "        for module, hook in module_forward_pre_hooks:\n",
    "            partial_hook = functools.partial(hook, **kwargs)\n",
    "            handles.append(module.register_forward_pre_hook(partial_hook))\n",
    "        for module, hook in module_forward_hooks:\n",
    "            partial_hook = functools.partial(hook, **kwargs)\n",
    "            handles.append(module.register_forward_hook(partial_hook))\n",
    "        yield\n",
    "    finally:\n",
    "        for h in handles:\n",
    "            h.remove()\n",
    "\n",
    "# ------------------ single-layer hook factories ------------------\n",
    "def get_direction_ablation_input_pre_hook(direction: Tensor):\n",
    "    def hook_fn(module, input):\n",
    "        nonlocal direction\n",
    "        activation = input[0] if isinstance(input, tuple) else input\n",
    "        direction  = direction / (direction.norm(dim=-1, keepdim=True) + 1e-8)\n",
    "        direction  = direction.to(activation)\n",
    "        activation -= (activation @ direction).unsqueeze(-1) * direction\n",
    "        return (activation, *input[1:]) if isinstance(input, tuple) else activation\n",
    "    return hook_fn\n",
    "\n",
    "def get_direction_ablation_output_hook(direction: Tensor):\n",
    "    def hook_fn(module, input, output):\n",
    "        nonlocal direction\n",
    "        activation = output[0] if isinstance(output, tuple) else output\n",
    "        direction  = direction / (direction.norm(dim=-1, keepdim=True) + 1e-8)\n",
    "        direction  = direction.to(activation)\n",
    "        activation -= (activation @ direction).unsqueeze(-1) * direction\n",
    "        return (activation, *output[1:]) if isinstance(output, tuple) else activation\n",
    "    return hook_fn\n",
    "\n",
    "def get_directional_patching_input_pre_hook(direction: Tensor, coeff: Float[Tensor, \"\"]):\n",
    "    def hook_fn(module, input):\n",
    "        nonlocal direction\n",
    "        activation = input[0] if isinstance(input, tuple) else input\n",
    "        direction  = direction / (direction.norm(dim=-1, keepdim=True) + 1e-8)\n",
    "        direction  = direction.to(activation)\n",
    "        activation -= (activation @ direction).unsqueeze(-1) * direction\n",
    "        activation += coeff * direction\n",
    "        return (activation, *input[1:]) if isinstance(input, tuple) else activation\n",
    "    return hook_fn\n",
    "\n",
    "def get_activation_addition_input_pre_hook(vector: Tensor, coeff: Float[Tensor, \"\"]):\n",
    "    def hook_fn(module, input):\n",
    "        nonlocal vector\n",
    "        activation = input[0] if isinstance(input, tuple) else input\n",
    "        vector     = vector.to(activation)\n",
    "        activation += coeff * vector\n",
    "        return (activation, *input[1:]) if isinstance(input, tuple) else activation\n",
    "    return hook_fn\n",
    "\n",
    "# ------------------ helper: build full-model hook lists -----------\n",
    "def get_all_direction_ablation_hooks(model_base, direction: Float[Tensor, \"d_model\"]):\n",
    "    fwd_pre = [\n",
    "        (model_base.model_block_modules[l],\n",
    "         get_direction_ablation_input_pre_hook(direction))\n",
    "        for l in range(model_base.model.config.num_hidden_layers)\n",
    "    ]\n",
    "    fwd_post = [\n",
    "        (model_base.model_attn_modules[l],\n",
    "         get_direction_ablation_output_hook(direction))\n",
    "        for l in range(model_base.model.config.num_hidden_layers)\n",
    "    ] + [\n",
    "        (model_base.model_mlp_modules[l],\n",
    "         get_direction_ablation_output_hook(direction))\n",
    "        for l in range(model_base.model.config.num_hidden_layers)\n",
    "    ]\n",
    "    return fwd_pre, fwd_post\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd9efc1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mechan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
